© 2026 Ji Hua.
This repository documents the Vibe + Coding methodology.
Licensed under CC BY-NC-ND 4.0.

Vibe + Coding — Version 0.2

# 第一章｜AI Coding 的真正问题：当决策与责任发生错位

在讨论任何流程、模板或“最佳实践”之前，我们必须先确立一个**不会随技术进步而改变的工程公理**：

**在任何真实的软件工程中，决策与责任不可分离。**
谁做出关键判断，谁就必须承担该判断在时间维度上所产生的长期后果。

这一点并不取决于工具是否先进，也不取决于执行是否高效。
它是工程系统得以长期存在的前提条件。

---

## 从一个公理出发

一旦承认“责任不可外包”，我们就必须面对一个直接而不舒服的推论：

> **凡是不承担工程后果的角色，都不具备在高责任判断中拥有最终决策权的合法性。**

正是这一点，构成了当今 AI Coding 的核心矛盾——
**AI 无法承担工程责任，却被默认参与了工程决策。**

大多数 AI Coding 在真实工程中的不稳定性，并非源于模型能力不足，
而是源于这一结构性错配。

---

## 为什么能力的提升解决不了这个问题

当前关于 AI 编程的讨论，几乎全部集中在能力层面：

- 模型是否足够聪明
- 上下文是否足够长
- prompt 是否足够精巧

但即便假设一个理想化前提——
AI 拥有无限上下文、完全理解需求、能够生成近乎完美的代码——
它依然无法解决一个根本问题：

**AI 无法为工程结果承担责任。**

在真实工程中，架构错误、技术债、维护成本和协作失败的后果，
最终只能由人类承担。
只要责任无法外包，最终决定权就不可能被彻底外包。

能力可以被外包，
判断可以被辅助，
但责任永远无法转移。

---

## 0 → 1 与 1 → 100：责任结构的断层

这一矛盾之所以在实践中长期被忽视，是因为人们混淆了两个责任结构完全不同的阶段。

当前主流 AI Coding 在 **0 → 1** 场景中表现极其出色：
快速把一个想法变成可运行的 demo、小工具或一次性脚本。

在这一阶段：

- 决策后果短暂
- 可以推倒重来
- 几乎没有长期维护义务

因此，即便 AI 在实现过程中隐式地做出一些判断，也不会立即引发结构性问题。

但真实的软件工程并不止步于此。
真正困难、也真正重要的是 **1 → 100**：

- 代码是否在数月后仍然可读
- 抽象是否能够承载变化
- 系统是否可以被他人接手
- 历史判断是否可追溯
- 错误是否可定位和修复

在这一阶段，每一个关键判断都会在时间维度上持续产生后果。
这并不是工程“复杂度”的增加，
而是**责任结构发生了质变**。

---

## 失效的并不是 AI，而是协作范式

这也正是为什么大量资深工程师对 AI Coding 持审慎态度。

他们并非否认 AI 的能力，
而是天然警惕责任体系被悄然破坏。

当前主流 AI Coding 隐含着一个几乎从未被系统性审视的假设：

> **人负责提需求，AI 负责代替完成。**

其结果是——
人在请求，
而 AI 在实现过程中**替代性地做出判断**。

在高责任工程中，这种模式必然导致：

- 判断在实现阶段悄然发生
- 设计取舍未被显式表达
- 原始意图被代码细节反向塑形
- 系统在没有清晰冻结点的情况下持续演化

最终失控的并不是代码是否“能跑”，
而是系统在不知不觉中，被一个不承担责任的执行体重塑。

雪上加霜的是，在主流的 AI 协作模式下，人类虽然获得了‘聊天的轻松体验’，却陷入了心智角色频繁切换的陷阱：

你必须在一瞬间是提出需求的产品经理，下一秒切换为决定架构的高级工程师，转头又要充当严苛审核代码的质检工程师。

人类工程师被迫在完全不同的认知尺度下进行高频决策。这种缺乏结构化约束的协作方式，不仅无法沉淀有效的工程判断，反而让工程师在‘被代码反向塑形’的过程中疲于奔命。

---

## 更强的 AI，只会放大这个问题

更值得警惕的是，更聪明的 AI 并不能解决这一问题，反而可能放大风险。

AI 越强，
越容易在实现过程中替你选择看起来“更优”的结构，
越容易在局部做出未经确认的取舍，
越容易在短期效率与长期责任之间，替你做出隐性权衡。

这些行为在短期内可能提升产出速度，
却同时模糊了判断边界，隐藏了责任归属，
使系统的演化路径不可控。

而 AI 永远无法为这些判断承担长期后果。

---

## 这是一个治理问题，而不是技术问题

这种责任错位并非 AI 编程特有。
它几乎一比一地复刻了人类组织中早已被反复验证的治理失败模式。

在任何成熟的组织中，
决策权的分配从来不是基于能力上限，
而是基于**失败后果是否仍然落在责任边界之内**。

一个不承担长期后果的角色，
即便在统计意义上“更可能正确”，
也不应被赋予高责任判断的最终裁量权。

工程世界并不特殊，AI 也不例外。

---

## 问题的本质与出路

因此，今天 AI Coding 的不稳定，并不是因为 AI 还不够强，
而是因为我们允许一个不承担责任的执行体，
在 **1 → 100 的工程中参与高责任判断**。

这是一个协作结构层面的错误，
而不是一个可以通过更好模型解决的问题。

解决这一问题的方向也因此变得清晰：

> **不是限制 AI 的能力，
> 而是重新对齐决策权与责任的归属。**

只要 AI 不承担工程后果，
它的判断权就必须被限制在与其责任相匹配的范围内；
而真正影响系统长期演化的判断，
必须由能够承担后果的一方做出。
